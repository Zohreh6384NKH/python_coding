{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large language models and how they work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### transformer architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pseudo code for the algorithm\n",
    "1- initialize the model with embedding, positional embedding, and transformer encoder layer and output layer.\n",
    "2- for each input sequence :\n",
    "a. get token embeddings from the embeddings layer\n",
    "b. create positional embedding based on sequence length (how many tokens the model can process at once ) and add them to the token embedding\n",
    "c. pass the combined embedding to the stacked transformer encoder layer\n",
    "    for each encoder layer:\n",
    "    a. apply multi head self attention\n",
    "    b.apply residual connection and layer normalization\n",
    "    c. pass through feed forward layer\n",
    "    d. apply residual connection and layer normalization\n",
    "d. pass the final encoder output to the output layer to get predictions\n",
    "3- return the output logits for each token position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 100, 1000])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_heads, num_layers, max_len = 5000):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        #embedding layer for input tokens\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        \n",
    "        # learn positional embedding\n",
    "        self.positional_embedding = nn.Embedding(max_len, embed_dim)\n",
    "        \n",
    "        # transformer encoder layer represent one layer of transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model = embed_dim,\n",
    "            nhead = num_heads,\n",
    "            dim_feedforward = 4 * embed_dim,\n",
    "            activation = 'relu'\n",
    "            \n",
    "        )\n",
    "        # a module that stacks multiple transformer encoder layers to form a complete encoder instead of manually stacking them\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers = num_layers)\n",
    "        \n",
    "        # fully connected output layer\n",
    "        self.fc_out = nn.Linear(embed_dim, input_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #apply embedding layer\n",
    "        x = self.embedding(x)\n",
    "        x = x*math.sqrt(self.embedding.embedding_dim) #caling the embeddings helps stabilize their values when theyâ€™re first fed into the Transformer layers.\n",
    "        #generate positional indices and apply positional embedding\n",
    "        seq_len = x.size(1)\n",
    "        positions = torch.arange(0, seq_len, device = x.device).unsqueeze(0)\n",
    "        \n",
    "        x = x + self.positional_embedding(positions)\n",
    "        # Pass through transformer encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        #pass through linear layer\n",
    "        output = self.fc_out(x)\n",
    "        return output\n",
    "            \n",
    "            \n",
    "input_dim = 1000  #vocabulary size\n",
    "embed_dim = 512   # embedding dimention\n",
    "num_heads = 8     # number of attention heads\n",
    "num_layers = 6    # number of transformer layers\n",
    "max_len = 5000    # number of sequence length\n",
    "\n",
    "model = TransformerModel(input_dim, embed_dim, num_heads, num_layers, max_len)\n",
    "\n",
    "# Sample input (batch_size, sequence_length)\n",
    "sample_input = torch.randint(0, input_dim, (32, 100))                     #sample input in the range of 0 to input_dim - 1 with shape (32, 100)\n",
    "\n",
    "output = model(sample_input)\n",
    "\n",
    "print(output.shape)  # Should output (batch_size, sequence_length, input_dim)\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n",
      "input token :tensor([[ 42,  24,  35,   3],\n",
      "        [ 51,   7, 100,  63]])\n",
      "output shape:torch.Size([2, 4, 64])\n",
      "torch.Size([32, 100])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "# we have a vocabulary of 10000 unique words (input_dim = 10000)\n",
    "\n",
    "# we want each word to be represented with 64-dimentional vector (embed_dim = 64)\n",
    "input_dim = 10000\n",
    "embed_dim = 64\n",
    "\n",
    "#initialize embedding layer\n",
    "embedding_layer = nn.Embedding(input_dim, embed_dim)\n",
    "example_input = torch.tensor([[42, 24, 35, 3], [51, 7, 100, 63]]) # a batch of two sentences each with 4 tokens\n",
    "print(example_input.shape)\n",
    "output = embedding_layer(example_input)\n",
    "print(f'input token :{example_input}')\n",
    "# print(f'embedded output:{output}')\n",
    "print(f'output shape:{output.shape}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
